{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e385cd52-83b7-4301-b037-919c3b9cf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fecd1da-3525-4e00-815d-ad3eb70b94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83535e7-3a8e-4f6c-a885-e3e7ec7ae123",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9990eb7-f76d-4205-9a87-4150e1bcd56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM-135M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5407c994-cfe3-421b-8517-742ef1a3f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d3b9c5-8bf6-4324-8207-5aa3e64758b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd2e68a-8d55-4027-afd4-0e08636c2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_string = tokenizer.decode([tokenizer.eos_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e38086-1bde-42a9-ba53-3f6abaccff28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1af7b11-953f-45b8-815c-30e62b112b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c9911c-0d45-4eff-aa00-8afe4d63d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7078cbc-ce9a-43f1-bd30-0b047d1bf008",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences.json', 'r', encoding='utf-8') as f:\n",
    "    text_for_ai = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c972536c-d268-437d-b311-e56cdb10ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"text\":text_for_ai})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe37b2e-d987-4e6a-aaeb-988042bde846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1947e8-ae2f-4281-b44d-ecc9ccbcb91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I read our tree\\n {\"I\": \"read\", \"our\": \"tree\"}<|endoftext|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e5a169b-1d8d-4f86-971f-9ea154896ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdb8bc16-b735-449d-beea-d2ef9773d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"],truncation=True,padding=\"max_length\",max_length=30,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b89cf39-ea65-45d7-85b6-6266355c4fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d200322d04145fc8f7a032c97f6b333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function,batched=True,remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a050480-c323-4dca-b044-47edd094dfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5357355b-f9de-4610-bf74-c31221f764ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89de09f5-55b9-452b-9d80-cfdec458b6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 19000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a1d0858-3050-45c4-8995-9612551ac47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "192c0f1b-712b-4f37-9862-cc75491efbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0642dc8-c938-4566-b00f-4f32c745a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data_collator([tokenized_dataset[\"train\"][i] for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "febcdb0c-12c9-43fa-b010-6f90beee8921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape:torch.Size([5, 30])\n",
      "attention_mask shape:torch.Size([5, 30])\n",
      "labels shape:torch.Size([5, 30])\n"
     ]
    }
   ],
   "source": [
    "for key in out:\n",
    "    print(f\"{key} shape:{out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa81eb8f-d76e-40d7-8686-ec10c4ccbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n",
    "    def __call__(self,examples):\n",
    "        batch = super().__call__(examples)\n",
    "        labels = batch['labels']\n",
    "        eos_token_id = self.tokenizer.eos_token_id\n",
    "        labels[labels == -100] = eos_token_id\n",
    "        batch['labels'] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "055f3524-19c8-4922-921c-463a2f51d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = CustomDataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c51ab57-c4f1-41b5-a4d2-5c0c68e75135",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data_collator([tokenized_dataset[\"train\"][i] for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9804c5bd-d85b-462f-aae8-d10546c4b84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11518, 10897,   957,  5460,   198,  9583, 11518,  1799,   476, 48112,\n",
       "         1002,   476,  2334,  1799,   476,  6733, 23597,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c15b82e8-c5d5-4f3d-bb09-395576eb5abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7341f089-ec51-49ef-94da-931574508e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"SmolLM\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-8,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c490293d-2865-408d-99a1-ce59d949604b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5205/4214465632.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e64df9b8-b3ab-4b07-b8c9-9ca9d1dd4013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2374' max='2374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2374/2374 3:03:08, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2374, training_loss=0.0, metrics={'train_runtime': 10993.0941, 'train_samples_per_second': 3.457, 'train_steps_per_second': 0.216, 'total_flos': 725972840110080.0, 'train_loss': 0.0, 'epoch': 1.9987368421052631})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d842746-ff54-49a5-8a0a-d284523c1619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small models are great.\n",
      "- The best models are those that are easy to use and that are easy to maintain.\n",
      "- The best models are those that are easy to use\n"
     ]
    }
   ],
   "source": [
    "trained_model = trainer.model\n",
    "prompt = \"Small models are great.\\n\"\n",
    "input_ids = tokenizer.encode(prompt,return_tensors=\"pt\",add_special_tokens=False).to(device)\n",
    "generated_ids = trained_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=30,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "generated_text=tokenizer.decode(generated_ids[0],skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "trained_model.save_pretrained(\"SmolLM-135M-fine-tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770068ae-6a14-42f1-b6af-6eaa5c426b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
